/*======
This file is part of Percona Server for MongoDB.

Copyright (C) 2023-present Percona and/or its affiliates. All rights reserved.

    This program is free software: you can redistribute it and/or modify
    it under the terms of the Server Side Public License, version 1,
    as published by MongoDB, Inc.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    Server Side Public License for more details.

    You should have received a copy of the Server Side Public License
    along with this program. If not, see
    <http://www.mongodb.com/licensing/server-side-public-license>.

    As a special exception, the copyright holders give permission to link the
    code of portions of this program with the OpenSSL library under certain
    conditions as described in each individual source file and distribute
    linked combinations including the program with the OpenSSL library. You
    must comply with the Server Side Public License in all respects for
    all of the code used other than as permitted herein. If you modify file(s)
    with this exception, you may extend this exception to your version of the
    file(s), but you are not obligated to do so. If you do not wish to do so,
    delete this exception statement from your version. If you delete this
    exception statement from all source files in the program, then also delete
    it in the license file.
======= */


#include "mongo/db/auth/authorization_session.h"
#include "mongo/db/catalog/import_options.h"
#include "mongo/db/catalog_raii.h"
#include "mongo/db/commands.h"
#include "mongo/db/commands/impexp_commands_gen.h"
#include "mongo/db/concurrency/d_concurrency.h"
#include "mongo/db/concurrency/exception_util.h"
#include "mongo/db/db_raii.h"
#include "mongo/db/op_observer/op_observer.h"
#include "mongo/db/storage/durable_catalog.h"
#include "mongo/db/storage/wiredtiger/wiredtiger_import.h"
// #include "mongo/idl/cluster_parameter_synchronization_helpers.h"
#include "mongo/logv2/log.h"

#define MONGO_LOGV2_DEFAULT_COMPONENT ::mongo::logv2::LogComponent::kCommand


namespace mongo {

namespace {

void buildStorageMetadata(const WTimportArgs& importArgs, BSONObjBuilder& bob) {
    bob << importArgs.ident
        << BSON("tableMetadata" << importArgs.tableMetadata << "fileMetadata"
                                << importArgs.fileMetadata);
}

// Import collection
void importCollection(OperationContext* opCtx, CollectionImportMetadata& metadata) {
    const auto importUUID = UUID::gen();

    BSONObjBuilder storageMetaBuilder;
    buildStorageMetadata(metadata.collection, storageMetaBuilder);
    for (auto&& index : metadata.indexes) {
        buildStorageMetadata(index, storageMetaBuilder);
    }
    const auto storageMetaObj = storageMetaBuilder.done();

    // Import the collection and its indexes.
    const auto nss = metadata.ns;
    writeConflictRetry(opCtx, "attachCollection", nss.ns(), [&] {
        LOGV2_DEBUG(29118, 1, "Attaching collection", "ns"_attr = nss);
        AutoGetDb autoDb(opCtx, nss.dbName(), MODE_IX);
        auto db = autoDb.ensureDbExists(opCtx);
        invariant(db);
        const Lock::CollectionLock collLock(opCtx, nss, MODE_X);
        auto catalog = CollectionCatalog::get(opCtx);
        WriteUnitOfWork wunit(opCtx);
        const AutoStatsTracker statsTracker(opCtx,
                                            nss,
                                            Top::LockType::NotLocked,
                                            AutoStatsTracker::LogMode::kUpdateTopAndCurOp,
                                            catalog->getDatabaseProfileLevel(nss.dbName()));

        // If the collection creation rolls back, ensure that the Top entry created for the
        // collection is deleted.
        opCtx->recoveryUnit()->onRollback(
            [nss, serviceContext = opCtx->getServiceContext()](OperationContext*) {
                Top::get(serviceContext).collectionDropped(nss);
            });

        // Create Collection object.
        auto storageEngine = opCtx->getServiceContext()->getStorageEngine();
        auto durableCatalog = storageEngine->getCatalog();
        ImportOptions importOptions(ImportOptions::ImportCollectionUUIDOption::kKeepOld);
        importOptions.importTimestampRule = ImportOptions::ImportTimestampRule::kStable;
        // Since we are using the ident id generated by this recipient node, ident collisions in
        // the future after import is not possible. So, it's ok to skip the ident collision
        // check. Otherwise, we would unnecessarily generate new rand after each collection
        // import.
        importOptions.skipIdentCollisionCheck = true;

        auto importResult = uassertStatusOK(DurableCatalog::get(opCtx)->importCollection(
            opCtx, nss, metadata.catalogObject, storageMetaObj, importOptions));

        const auto md = durableCatalog->getMetaData(opCtx, importResult.catalogId);
        for (const auto& index : md->indexes) {
            uassert(6114301, "Cannot import non-ready indexes", index.ready);
        }

        std::shared_ptr<Collection> ownedCollection = Collection::Factory::get(opCtx)->make(
            opCtx, nss, importResult.catalogId, md, std::move(importResult.rs));
        ownedCollection->init(opCtx);
        ownedCollection->setCommitted(false);

        // Update the number of records and data size on commit.
        opCtx->recoveryUnit()->registerChange(
            makeCountsChange(ownedCollection->getRecordStore(), metadata));

        CollectionCatalog::get(opCtx)->onCreateCollection(opCtx, std::move(ownedCollection));

        auto importedCatalogEntry =
            storageEngine->getCatalog()->getCatalogEntry(opCtx, importResult.catalogId);
        opCtx->getServiceContext()->getOpObserver()->onImportCollection(opCtx,
                                                                        importUUID,
                                                                        nss,
                                                                        metadata.numRecords,
                                                                        metadata.dataSize,
                                                                        importedCatalogEntry,
                                                                        storageMetaObj,
                                                                        false);

        wunit.commit();

        // TODO: remove if not necessary
        // if (metadata.numRecords > 0) {
        //     cluster_parameters::maybeUpdateClusterParametersPostImportCollectionCommit(opCtx,
        //     nss);
        // }

        LOGV2(29119,
              "Attached collection",
              "ns"_attr = nss,
              "numRecordsApprox"_attr = metadata.numRecords,
              "dataSizeApprox"_attr = metadata.dataSize);
    });
}

}  // namespace

/**
 * Example attachCollection command:
 *   {
 *       attachCollection: {
 *          ns: "test.foo",
 *          metadata: {
 *              ...
 *          },
 *          numRecords: Long("1"),
 *          dataSize: Long("46"),
 *          storageMetadata: {
 *              'collection-7--6037007956219898235': {
 *                  tableMetadata: ' ... ',
 *                  fileMetadata: ' ... '
 *              },
 *              'index-8--6037007956219898235': {
 *                  tableMetadata: ' ... ',
 *                  fileMetadata: ' ... '
 *              }
 *          },
 *       }
 *   }
 */
class AttachCollectionCmd : public BasicCommand {
public:
    AttachCollectionCmd() : BasicCommand("attachCollection") {}

    AllowedOnSecondary secondaryAllowed([[maybe_unused]] ServiceContext* context) const override {
        return AllowedOnSecondary::kAlways;
    }

    bool supportsWriteConcern([[maybe_unused]] const BSONObj& cmd) const override {
        return false;
    }

    Status checkAuthForOperation(OperationContext* opCtx,
                                 const DatabaseName& dbName,
                                 const BSONObj& cmdObj) const override {
        auto* as = AuthorizationSession::get(opCtx->getClient());
        if (!as->isAuthorizedForActionsOnResource(parseResourcePattern(dbName, cmdObj),
                                                  ActionType::importCollection)) {
            return {ErrorCodes::Unauthorized, "unauthorized"};
        }

        return Status::OK();
    }

    bool run(OperationContext* opCtx,
             const DatabaseName& dbName,
             const BSONObj& cmdObj,
             BSONObjBuilder& result) override {
        auto request = AttachCollectionRequest::parse(IDLParserContext("attachCollection"), cmdObj);
        auto const& params = request.getCommandParameter();

        // Populate collection import metadata
        CollectionImportMetadata metadata;
        metadata.catalogObject = params.getMetadata();
        const BSONObj& md = metadata.catalogObject;
        auto const& storageMetadata = params.getStorageMetadata();

        metadata.collection.ident = md["ident"].String();
        {
            auto md = storageMetadata[metadata.collection.ident].Obj();
            metadata.collection.tableMetadata = md["tableMetadata"].String();
            metadata.collection.fileMetadata = md["fileMetadata"].String();
        }
        metadata.ns = params.getNs();
        metadata.numRecords = params.getNumRecords();
        metadata.dataSize = params.getDataSize();
        LOGV2_DEBUG(29120,
                    1,
                    "Attach collection metadata",
                    "ns"_attr = metadata.ns,
                    "ident"_attr = metadata.collection.ident,
                    "tableMetadata"_attr = metadata.collection.tableMetadata,
                    "fileMetadata"_attr = metadata.collection.fileMetadata);

        // Collect indexes metadata
        auto idxIdent = md["idxIdent"].Obj();
        for (auto idx : idxIdent) {
            WTIndexImportArgs impArgs;
            impArgs.indexName = idx.fieldName();
            impArgs.ident = idx.String();
            auto md = storageMetadata[impArgs.ident].Obj();
            impArgs.tableMetadata = md["tableMetadata"].String();
            impArgs.fileMetadata = md["fileMetadata"].String();
            metadata.indexes.push_back(impArgs);
        }

        importCollection(opCtx, metadata);
        return true;
    }

} attachCollectionCmd;
}  // namespace mongo
